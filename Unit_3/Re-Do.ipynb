{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing DataFrame and removing columns/null-values that created index problems\n",
    "# (similar to Guided Example DF):\n",
    "\n",
    "data = pd.read_csv(\"LoanStats3d.csv\", skipinitialspace = True, header = 1, engine = 'python', skipfooter = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning:\n",
    "\n",
    "data.drop(['member_id', 'id', 'url', 'emp_title', 'zip_code', 'earliest_cr_line', 'revol_util',\n",
    "            'sub_grade', 'addr_state', 'desc'], 1, inplace=True)\n",
    "\n",
    "# Giving the 'loan_status' column some values that we can refer to later:\n",
    "\n",
    "data.replace({\"loan_status\":{\"Charged Off\": 0,\n",
    "                             \"Current\": 1,\n",
    "                             \"Default\":  2,\n",
    "                             \"Fully Paid\":  3,\n",
    "                             \"In Grace Period\":  4,\n",
    "                             \"Late (16-30 days)\":  5,\n",
    "                             \"Late (31-120 days)\":  6}}, inplace = True)\n",
    "\n",
    "data.loan_status.astype('int64')\n",
    "data.drop(data.select_dtypes(include = ['object']).keys(), axis = 1, inplace = True)\n",
    "data.dropna(how = 'any', axis = 1, inplace = True)\n",
    "\n",
    "# Making a copy of the df so we can assign one to the target data (with loan status info) and one for the training\n",
    "# data without the loan status info.\n",
    "\n",
    "data2 = data.copy()\n",
    "\n",
    "# Dropping the last two problematic rows:\n",
    "\n",
    "data2 = data2[:-2]\n",
    "data = data[:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Trees:\n",
    "\n",
    "Starting our complex tree here to compare it to the forest model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop(columns = ['out_prncp', 'loan_status',\n",
    "       'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp',\n",
    "       'total_rec_int', 'total_rec_late_fee', 'recoveries',\n",
    "       'collection_recovery_fee', 'last_pymnt_amnt'], axis = 1)\n",
    "\n",
    "y = data['loan_status']\n",
    "\n",
    "class_list = [\"Charged Off\", \"Current\", \"Default\", \"Fully Paid\", \"In Grace Period\", \n",
    "                \"Late (16-30 days)\", \"Late (31-120 days)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Default Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going with the default tree first (limited max_depth to 10 for runtime considerations):\n",
    "start_time = datetime.now()\n",
    "default_tree = tree.DecisionTreeClassifier(criterion = 'gini', \n",
    "                                           max_depth = 10, \n",
    "                                           max_features = None)\n",
    "\n",
    "default_tree.fit(x, y)\n",
    "\n",
    "# Graph:\n",
    "from IPython.display import Image\n",
    "import pydotplus.graphviz\n",
    "\n",
    "dot_data = tree.export_graphviz(default_tree, out_file = None, \n",
    "                                feature_names = x.columns, \n",
    "                                class_names = class_list, filled=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score is: 0.5954599102811018. \n",
      " The runtime is:0:00:24.468823\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy score is: {}. \\n The runtime is:{}\".format(default_tree.score(x, y), (end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default code will let the tree run until until it has found the best classification for the datapoints (no matter how deep or how many splits. That is why you will get a graph such as the one above, where the graph is so complex (i.e. number of splits, depth and therefore leaves), that it is hard to read unless you export the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### First Tree:\n",
    "> Making a tree with slightly more depth than the default and also specifying the number of leaf samples and leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model:\n",
    "start_time = datetime.now()\n",
    "\n",
    "tree_1 = tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                     max_depth = 16, \n",
    "                                     max_features = None,\n",
    "                                     min_samples_split = 200,\n",
    "                                     min_samples_leaf = 70,\n",
    "                                     max_leaf_nodes = 60)\n",
    "\n",
    "tree_1.fit(x, y)\n",
    "\n",
    "# Graph\n",
    "dot_data = tree.export_graphviz(tree_1, out_file = None,\n",
    "                                feature_names = x.columns, \n",
    "                                class_names = class_list,\n",
    "                                filled=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score is: 0.5814796256408917. \n",
      " The runtime is:0:00:09.116010\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy score is: {}. \\n The runtime is:{}\".format(tree_1.score(x, y), (end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters on this tree are a little more specific.  While the accuracy has gone done, we are at least able to get a visual of the classification process.  Controlling the max_leaf_nodes is probably the most helpful in getting the visual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Second Tree:\n",
    "> Respecifying the parameters in the hopes of getting a more diverse representation of the classes for 'loan-status' and/or a higher accuracy score.  I increased the max_depth, removed the min_samples_split to give it more flexibility, as well as decreasing the min_sample_leaf amount.  I allowed for more leaf nodes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model:\n",
    "start_time = datetime.now()\n",
    "\n",
    "tree_2 = tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                     max_depth = 18, \n",
    "                                     max_features = None,\n",
    "                                     min_samples_leaf = 100,\n",
    "                                     max_leaf_nodes = 70)\n",
    "tree_2.fit(x, y)\n",
    "\n",
    "# Graph:\n",
    "dot_data = tree.export_graphviz(tree_2, out_file = None,\n",
    "                                feature_names = x.columns,\n",
    "                                class_names = class_list,\n",
    "                                filled=True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "Image(graph.create_png())\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score is: 0.5817764721807297. \n",
      " The runtime is:0:00:08.912194\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy score is: {}. \\n The runtime is:{}\".format(tree_2.score(x, y), (end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you can still see the class imbalance between 'Fully Paid' and 'Current' loans (especially given that these are the only two out of six categories we see).  The accuracy score has increased a very miniscule amount and the runtime has decreased by about a third.  In order to increase the accuracy, we would have to allow for more room for depth and leaves, which would make it difficult to create a visual.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Forests:\n",
    "> Using the default constraints here, but limiting the depth for runtime purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "default_forest = ensemble.RandomForestClassifier(max_depth = 10)\n",
    "\n",
    "default_forest.fit(x, y)\n",
    "\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score is: 0.571859422977822. \n",
      " The runtime is:0:00:10.150381\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy score is: {}. \\n The runtime is:{}\".format(default_forest.score(x, y), (end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Forest 1\n",
    "Aiming towards simplicity here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "forest_1 = ensemble.RandomForestClassifier(n_estimators = 8, \n",
    "                                           criterion = 'entropy',\n",
    "                                           max_features = 4,\n",
    "                                           max_depth = 6,\n",
    "                                           min_samples_split = 5000,\n",
    "                                           min_samples_leaf = 1000,\n",
    "                                           max_leaf_nodes = 40)\n",
    "\n",
    "forest_1.fit(x, y)\n",
    "\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score is: 0.5549011738499572. \n",
      " The runtime is:0:00:04.549697\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy score is: {}. \\n The runtime is:{}\".format(forest_1.score(x, y), (end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to the default model, the accuracy has only decreased by two percent, yet the runtime has decreased by fifty percent, which is an interesting consideration. In comparison to the best decision tree model above, we are getting only a five percent decrease in accuracy for a significantly more reliable accuracy score at an eighth of the runtime. We do not, however, get the visual which could be useful for certain phases of the analysis/study in question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now()\n",
    "\n",
    "forest_2 = ensemble.RandomForestClassifier(n_estimators = 4, \n",
    "                                           criterion = 'entropy',\n",
    "                                           max_features = 3,\n",
    "                                           max_depth = 3,\n",
    "                                           min_samples_split = 2000,\n",
    "                                           min_samples_leaf = 500,\n",
    "                                           max_leaf_nodes = 20)\n",
    "\n",
    "forest_2.fit(x, y)\n",
    "\n",
    "end_time = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy score is: 0.5548821756714075. \n",
      " The runtime is:0:00:01.375754\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean accuracy score is: {}. \\n The runtime is:{}\".format(forest_2.score(x, y), (end_time-start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, our accuracy has decreased only minimally and the runtime has decreased by a fourth.  You can see here that the model has reached a point where it starts to plateau in terms of changing parameters and accuracy.  The most significant change was in runtime.  Thus, for half the number of estimatores, half the leaves, you get the same accuracy and one fourth of the runtime.  In comparison to the tree models, where the reliability of your accuracy score is weaker than four trees in this last forest model, this model is quite powerful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
